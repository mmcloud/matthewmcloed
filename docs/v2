DORA Metrics Plugin for Harness – Requirements Specification

Functional Requirements
	•	DORA Metrics Computation: The plugin shall compute and display the five key DevOps Research and Assessment (DORA) metrics for software delivery performance – Deployment Frequency, Lead Time for Changes, Change Failure Rate, Mean Time to Recovery (MTTR), and Rework Rate. These metrics are defined according to industry standards, for example:
	•	Deployment Frequency: How often new code is deployed to production (e.g. deployments per day or week) ￼.
	•	Lead Time for Changes: The time from code commit to successful deployment in production (indicates the velocity of the delivery pipeline) ￼.
	•	Change Failure Rate: The percentage of deployments that result in a failure (e.g. require a rollback or hotfix due to production issues) ￼.
	•	Mean Time to Recovery (MTTR): The average time to restore service after a production failure (measured from incident start to resolution) ￼.
	•	Rework Rate: The percentage of work that must be redone due to defects or unplanned fixes (recently introduced as a 5th DORA metric in 2024) ￼.
	•	Data Collection and Aggregation: The plugin shall collect necessary data from multiple systems to calculate the above metrics, since the relevant information is spread across tools (source code, deployments, change tickets, and incidents). For this plugin: GitHub will provide commit and deployment information, Jira will provide change tracking (work items) and assist in linking changes to incidents, and ServiceNow will provide change management records and incident data. (In practice, DevOps metrics data is often distributed across several tools and must be stitched together to get a full picture ￼.) The plugin must aggregate data from these sources to enable end-to-end traceability of work from code commit through deployment to incident resolution.
	•	Integration Data Mapping: The system must correctly map and correlate data across GitHub, Jira, and ServiceNow to enable accurate metric calculations. For example, it should match deployment events to the commits that were deployed (using commit SHAs or tags) and possibly to a change ticket ID:
	•	When a Harness pipeline execution occurs, the plugin should capture the commit ID, build/deploy time, and status from Harness/GitHub (Harness pipelines already provide commit details and timestamps for each execution ￼). This mapping allows linking a code commit to its deployment for Lead Time calculations.
	•	The plugin should correlate deployments with change records or issues. This could involve parsing commit messages for Jira issue keys or using a custom linkage (e.g. each Jira ticket may store a ServiceNow Change ID) to tie development work to deployment events ￼. For instance, if a Jira user story is linked to a ServiceNow Change record, the plugin can use that link to calculate Lead Time as “change resolved date – story creation date” in addition to the standard commit-to-prod metric ￼ (providing flexibility in how lead time is defined).
	•	The plugin should link incident records to deployments. This may be done by matching a ServiceNow incident’s cause or start time with a specific deployment (or by a reference field if the incident or change ticket references a deployment ID). By correlating incidents to the responsible deployment, the plugin can mark that deployment as a “failed change” for Change Failure Rate calculation, and use the incident’s resolution time to contribute to MTTR.
	•	Metrics Calculation Rules: The plugin shall implement the logic to compute each metric from the collected data:
	•	Deployment Frequency (DF): Calculate the number of successful deployments to production in a given period (e.g. per day, week, month) ￼. This requires counting pipeline deployments (filtered to production environments) over the time window. The system should allow configurable aggregation periods (daily, weekly, etc.) for DF. Each service/environment combination should be tracked separately (Harness’s best practices capture metrics per service-environment combination ￼) and also allow an organization-wide aggregate.
	•	Lead Time for Changes: Compute the time difference between when a change is introduced and when it is deployed. By default this will be measured from code commit timestamp to the deployment completion time in production ￼. The plugin should calculate an average or median lead time over a period. It should handle multiple commits per deployment (e.g. use the commit that initiated the deploy or the last commit included). Optionally, the plugin may support alternative lead time definitions (configurable) – for example from when a Jira issue is created or marked “in progress” to when its code is deployed – to accommodate different interpretations of lead time ￼.
	•	Change Failure Rate (CFR): Determine the percentage of deployments that result in a failure in production ￼. The plugin will identify “failed” deployments by either pipeline outcomes or post-deployment incident data. For example, a deployment can be flagged as failed if a ServiceNow Change record related to it is marked unsuccessful or failed, or if a P1/P2 incident in ServiceNow is linked to that deployment shortly after release. Using ServiceNow Change data, the plugin can calculate CFR as number of failed changes / total changes in the period ￼, applying whatever failure criteria the organization defines (e.g. change ticket marked as failed, or an incident occurred within X hours of deployment).
	•	Mean Time to Recovery (MTTR): Compute the average time to restore service after a failure, typically by using incident data ￼. The plugin should take each relevant incident (e.g. Sev-1 incidents in ServiceNow, or incidents linked to deployment failures) and measure the duration from incident start to resolution. It will then average these durations over the reporting period. If the organization wants to focus on critical incidents, the plugin should allow filtering by incident priority (for example, focusing on Priority 1 incidents for MTTR calculations as a key indicator of reliability) ￼.
	•	Rework Rate: Calculate the percentage of work that had to be redone due to defects or failures ￼. This could be measured as the proportion of hotfix or patch deployments that were unplanned. For instance, if a deployment failed and required an immediate fix (another deployment), that second deployment is “rework.” The plugin might derive rework rate by tracking how many deployments in a period were unplanned or emergency fixes versus planned releases (or by using a flag in tickets/commits marking them as fixes to recent issues). It could also consider code churn metrics – e.g. the percentage of code changes in a release that modify recent code (indicating rework). The exact formula should be clearly defined (aligned with DORA’s 2024 report definition of rework rate ￼) and configurable to the organization’s interpretation.
	•	User Stories and Functional Behaviors:
Data Viewing: As a team lead or engineer, I want to view the DORA metrics (DF, Lead Time, CFR, MTTR, Rework) for my project so that I can understand our performance and identify areas for improvement. The plugin will provide a dashboard showing these metrics for the services I care about, with the ability to drill down for more details on each metric.
Data Refresh: As an executive or manager, I want the metrics to be updated regularly (e.g. daily) and reflect the most recent deployments and incidents, so that our reports are up-to-date. The plugin will automatically collect data on a scheduled basis and update the metrics, but also allow on-demand refresh if I need the latest data immediately.
Cross-Tool Traceability: As a DevOps engineer, I want to trace a high Change Failure Rate back to the specific deployments and incidents causing failures. The plugin will enable me to click on a metric (like CFR) and see which deployments were counted as failures and access links to the corresponding incident tickets in ServiceNow and commit information in GitHub. This traceability across GitHub, Jira, and ServiceNow helps diagnose problems quickly.
	•	User Access and Permissions: The plugin must respect access controls. Only authenticated users within the organization can view the DORA metrics, and if necessary, the plugin should integrate with Backstage’s authentication/authorization system to restrict data by team or role. For example, users might only see metrics for services they own, whereas an admin or executive role could view aggregate metrics for all services. (Backstage’s auth system supports identifying users and delegating access to third-party resources on their behalf ￼, which can be leveraged for permission control if needed.) All access to external data (GitHub, Jira, ServiceNow) should also comply with those systems’ permissions – e.g. if a user does not have rights to certain Jira projects or ServiceNow records, the plugin should handle that (possibly by using service accounts or having the user’s own credentials scope what they can see).
	•	Configurability: The plugin shall allow configuration of key parameters: which projects/repos and environments to include, which Jira projects or filters to use, ServiceNow instance details, etc. For example, a configuration might map a Backstage service entity to a GitHub repository (or Harness project) and a Jira project key, plus a ServiceNow application or service identifier. This mapping ensures the plugin knows which data to pull for each service’s dashboard. It should be easy to add or change these mappings (through a config file or admin UI) so that new services can start collecting metrics with minimal setup. (In Harness’s Backstage plugin for pipelines, for instance, the user links a Backstage service to a Harness project to view pipeline executions ￼; similarly, this DORA plugin could use annotations on Backstage catalog entities to configure repository names, Jira keys, and ServiceNow identifiers for that service.)
	•	Real-Time and Batch Updates: The system should support a reasonable update frequency for metrics. It will likely operate on a near-real-time pull model (periodically polling the APIs) or a push model via webhooks:
	•	If possible, utilize webhooks or event hooks for immediate data updates (e.g. GitHub can send a webhook on each code push or deployment event, which the plugin backend can receive to update metrics incrementally ￼). Jira can send webhooks on issue transitions, and ServiceNow could send outbound messages on ticket updates – these would reduce latency in metric updates.
	•	In addition, or alternatively, schedule regular polling of each system’s API (for example: check every X minutes for new deployments, new commits merged, new incidents resolved) to catch any data missed by webhooks or for systems where webhooks are not available. Polling intervals should be configurable per integration to balance timeliness and API rate limits.
	•	The plugin should aggregate data in a way that avoids duplication – e.g. maintain timestamps of last processed events for each integration, so it only fetches new or updated records on each cycle.
	•	Error Handling and Data Quality: If any data source is temporarily unavailable or returns an error, the plugin should handle it gracefully. For instance, if ServiceNow’s API is down, the dashboard could warn that incident data is unavailable (rather than showing stale or zero values) and fall back to the last known metrics. The system should log integration errors for troubleshooting but surface only user-friendly notifications in the UI. It should also validate data for consistency (e.g. ensure timestamps make sense, filter out any obviously incorrect data points) so that metric calculations remain accurate. Any assumptions in calculations (such as ignoring weekends for Lead Time, or only counting business hours for MTTR, if applicable) should be documented and configurable.

Non-Functional Requirements
	•	Performance: The plugin must compute and serve metrics efficiently. The metrics dashboard should load within a few seconds at most, even when processing data for large projects (e.g. hundreds of deployments and thousands of commits in a time period). To achieve this, the implementation may use caching or pre-computation of metrics (for example, maintaining a running tally in a database) rather than recalculating everything on every page load. Data queries to external systems should be optimized (using pagination, filters, and only retrieving necessary fields). The plugin’s presence should not significantly degrade Backstage’s overall performance; background data fetch jobs should be scheduled to avoid peak usage times if needed.
	•	Scalability: The solution should scale to many teams and high data volumes. It must support dozens of services/projects and potentially multiple GitHub organizations or Jira projects. As the number of commits, tickets, and incidents grows, the system should handle the load by scaling its data storage or processing (for example, using a persistent store or time-series database for metrics). The design should allow horizontal scaling of the plugin’s backend component if necessary (e.g. running multiple instances for load balancing). Additionally, adding a new integration (say, Bitbucket or GitLab instead of GitHub in the future) or new metrics should be feasible by extending the plugin rather than a complete rewrite, demonstrating good modularity and maintainability.
	•	Security: Security is paramount since the plugin will integrate with sensitive developer and incident data. All communication with GitHub, Jira, and ServiceNow APIs must be over HTTPS and use secure authentication methods (API tokens or OAuth 2.0 flows as supported by each). Credentials for these integrations should be stored securely (e.g. encrypted at rest in configuration or within a secrets manager). The plugin will follow the principle of least privilege – for example, use read-only API scopes and only access repositories or projects necessary for the metrics. No sensitive data from the integrations (such as source code or customer data in incident descriptions) should be persisted unnecessarily in the plugin’s storage. Also, the plugin should sanitize and guard any data it displays: for instance, if incident titles or Jira issue names are shown, ensure no confidential personal information is exposed on the dashboard without authorization. All user interactions will be subject to Backstage’s security model, and the plugin itself should be reviewed for common vulnerabilities (e.g. SQL injection if it uses a database, or injection attacks via any data it might display).
	•	Reliability and Availability: The plugin should be designed for high availability. It should handle network or service outages gracefully (as noted in error handling) and recover automatically when external systems come back online. Any background processing should be robust – e.g. if a data fetch fails mid-run, it should retry or resume later. The system should also avoid single points of failure: if one integration (say Jira) is slow, it should not block processing data from the others. In terms of uptime, the goal is that the metrics dashboard is available to users whenever Backstage is up, with the understanding that some data might be stale if external services are down. Logging and monitoring should be in place for the plugin’s operations, so that if it fails or data becomes inconsistent, engineers can quickly detect and fix the issue. Ideally, the plugin could emit its own health metrics (e.g. last successful sync time for each system) to be monitored by the platform team.
	•	Maintainability: The codebase of the plugin should be modular and well-documented. Each integration (GitHub, Jira, ServiceNow) should be implemented as a separate module or service, with clear interfaces, so they can be modified or replaced independently. The metrics calculation logic should also be separated from data retrieval logic, allowing easy changes (for example, if the definition of a metric needs adjustment, or if a new metric is added). There should be unit and integration tests covering the computation of metrics (e.g. test that given sample data, the CFR or MTTR is calculated correctly). Configuration should be externalized (e.g. in Backstage app-config or environment variables) rather than hard-coded, to ease maintenance and updates (for instance, updating API endpoints or credentials should not require code changes). The plugin should come with setup instructions and usage documentation so that platform administrators know how to configure integrations and developers understand how to use the dashboard.
	•	Compliance: The design must take into account any compliance and regulatory requirements relevant to the data. For example, if ServiceNow incident data includes personal data or is subject to ITIL change management audit, the plugin should not violate data retention rules – it might avoid storing full incident details and instead query them on the fly, or purge historical data after a certain period. If the organization has policies like SOC2, GDPR, or internal data classification standards, the plugin’s handling of data should align with those (ensuring that sensitive data is encrypted in transit and at rest, and that access to the data is audited). Additionally, the plugin should respect the usage terms of the integrated platforms (GitHub, Jira, ServiceNow) – for instance, staying within API usage limits and not exposing data in ways disallowed by those services. By adhering to compliance and platform guidelines, the plugin will be sustainable and approvable by governance teams.
	•	Usability: (Note: Though not explicitly requested, usability is a key quality aspect.) The dashboard should be intuitive for users who may not be experts in DORA metrics. Non-functional but important, it should present information in a clear manner and provide guidance (such as the tooltips mentioned below, documentation links, or consistent UI patterns). This ensures the insights from the metrics can be acted upon by teams, fulfilling the plugin’s ultimate purpose of driving improvement in DevOps performance.

Integration Requirements
	•	GitHub Integration: The plugin will integrate with GitHub (or GitHub Enterprise) to gather source control and deployment data. It should support authenticating to GitHub via a Personal Access Token or OAuth app with appropriate scopes (e.g., access to repository metadata and possibly deployment statuses). Once authenticated, the plugin will use GitHub’s APIs to retrieve relevant data:
	•	Commit Data: For repositories of interest, fetch commit history and metadata. Specifically, the plugin may need the timestamps of commits (or merge events for pull requests) that get deployed. This helps calculate Lead Time for Changes by comparing commit time to deployment time.
	•	Deployment Events: If available, use GitHub’s Deployment API or status checks. In many CI/CD setups (especially if using Harness), GitHub can be updated with deployment statuses. The plugin should leverage any deployment markers in GitHub (for example, if Harness pipelines create GitHub Deployment records or statuses on commits) to know when a commit was deployed. If such records are not available, the plugin will rely on Harness or ServiceNow change data for deployment events.
	•	Webhooks: The integration should allow GitHub webhooks to push events to the plugin’s backend. For instance, a push or PR merge event could trigger the plugin to note new commits, and a deployment event could trigger calculating a new deployment frequency count ￼. Using webhooks can minimize delays and API polling. The plugin’s backend would need an endpoint to receive these events securely (with signature verification from GitHub).
	•	Data Filtering: The integration must know which repositories and branches to monitor (likely through configuration). For example, it might only track the main or master branch of certain repos for production deployments. It should ignore irrelevant repos or branches to reduce noise.
	•	Rate Limiting: GitHub API has rate limits; the plugin should handle this by spacing API calls or using conditional requests (e.g., ETags to only fetch if data changed). For on-premises GitHub Enterprise, it should similarly respect any performance considerations.
	•	Jira Integration: The plugin will connect to Jira (likely Jira Cloud or Server via REST API) to pull work item and change information:
	•	Authentication: Use an API token with Basic Auth (for Jira Cloud) or OAuth if available. The credentials should be for a service account that has read access to the relevant Jira projects.
	•	Data Retrieval: Fetch issues relevant to deployments and incidents. This could include user stories, bugs, or tasks that represent “changes” being delivered. Key fields to retrieve might be issue keys, statuses, creation date, resolution date, and any custom fields linking to other systems (e.g., a field that stores a ServiceNow Change ID or an incident ID if the workflow links them).
	•	Change Tracking: The plugin should utilize Jira to track the progress of changes. For example, if measuring lead time from idea to production, the Jira issue’s creation or “start progress” date can serve as the start, and linking it to a deployment serves as the end. In one scenario, each Jira issue could have a custom field for the ServiceNow Change request number that implemented it ￼; the plugin should be able to use such links if present. If not, it may fall back to matching by issue key in commit messages.
	•	Incident Linkage: Jira might also be used by development teams to track incidents or problem reports (even if ServiceNow is the ITSM source of truth). If the organization has a practice of creating Jira tickets for production incidents or post-mortems, the plugin should incorporate that. For instance, a ServiceNow incident could be referenced in a Jira bug ticket; the plugin can then connect that Jira bug to the original incident and the deployment that caused it. This helps provide context on rework or failure causes in the metrics.
	•	Polling/Webhooks: Similar to GitHub, the plugin can use Jira webhooks (Jira can send events on issue updates) to know when an issue is created or transitioned (e.g., to “Done” or to “Deployed” if such a status exists). Otherwise, the plugin should poll Jira periodically for changes in relevant issues (filtering by update date or JQL queries to find recent changes).
	•	Mapping to Services: The integration should know which Jira projects (or issue filters) correspond to which service or team. This might be configured by naming conventions or explicit mapping in Backstage metadata (e.g., an annotation on a service entity like jiraProject: KEY). The plugin should use that to query only pertinent issues for a given context.
	•	Data usage: Data from Jira will feed primarily the Lead Time metric (by providing the “start” of work) and possibly Rework Rate (if we count reopened issues or unplanned work). It might also indirectly feed Change Failure Rate if the team tracks failed changes in Jira (though that’s more likely via incidents in ServiceNow).
	•	ServiceNow Integration: The plugin will integrate with ServiceNow to gather change management and incident management data:
	•	Authentication: Use ServiceNow’s REST API (Table API) with a service account. Authentication could be basic (username/password) or OAuth/client credentials depending on what the ServiceNow instance supports. Ensure the account has read access to the relevant tables (e.g., Incident [incident] and Change Request [change_request] tables).
	•	Change Management Data: The plugin should fetch records of changes/deployments from ServiceNow. Many organizations log production deployments as Change Requests in ServiceNow for approval and tracking. The plugin will use these records to identify how many deployments were executed and which succeeded or failed. Key fields to retrieve include the change request number, status or outcome (e.g., Implemented vs Backed Out), implemented date/time, and perhaps a link to a CI/CD pipeline or Jira ticket if available. For example, the plugin can query for all Change Requests in a given time window that correspond to production deployments and count how many were successful vs failed to compute Change Failure Rate ￼. If ServiceNow has a field for “deployment plan” or references a specific pipeline, that can help correlate to GitHub/Harness data.
	•	Incident Data: The plugin will retrieve incident records to calculate MTTR (and influence CFR and Rework). Focus should be on production incidents (for instance, filter by Priority or Category to identify relevant incidents – e.g., Priority 1 and 2 incidents that signify outages or major issues). Important fields are incident ID/number, opened timestamp, resolved timestamp, and if possible, a field linking the incident to a change or root cause. Often, ServiceNow incidents have a field like Cause Change or a reference to a Change Request that caused the incident. The plugin should use such linkage if present to connect an incident to the deployment (change) that caused it. In the absence of an explicit link, the plugin might use time correlation (an incident that started soon after a deployment) as a heuristic. The integration will then compute MTTR by taking the difference between opened and resolved times for each incident and averaging them ￼ ￼.
	•	Data Volume and Filtering: The plugin should query ServiceNow with filters to limit to relevant data. For example, use a query to fetch incidents of certain priority in the last X days, or changes of a certain type (normal/emergency) in a date range. This prevents pulling the entire ServiceNow dataset. Since ServiceNow can contain data across the whole company, the integration should ideally filter by a specific application service or CI that corresponds to the software service in question. This may require that each Backstage service is associated with a ServiceNow Configuration Item or Service identifier, which the plugin would use in queries (e.g., get incidents where cmdb_ci = “MyService”). This mapping would be part of the configuration.
	•	Polling Strategy: ServiceNow might not easily push events (unless using MID servers or business rule hooks), so the plugin will likely poll the ServiceNow API on a schedule. It should use the ServiceNow query capabilities to only retrieve records updated since the last poll (e.g., using the sys_updated_on field to get incremental changes). Polling frequency could be on the order of every 5 or 15 minutes for incidents (to update MTTR quickly), and maybe daily for change records (since deployment counts can be updated less frequently). These intervals should be configurable.
	•	API Limits and Performance: The plugin should be mindful of ServiceNow API limits and performance. ServiceNow may restrict the number of results per query (pagination via sysparm_limit) – the plugin must handle paging through results if needed. Also, to avoid heavy load, it might retrieve only needed fields (using sysparm_fields). If large volumes of data need processing (e.g., calculating yearly averages), consider offloading that to the plugin’s own database after an initial sync.
	•	Unified Data Correlation: The plugin’s integration layer must unify the data from GitHub, Jira, and ServiceNow for metric calculations. Some specific requirements for this correlation:
	•	There shall be an internal data model (or schema) that the plugin uses, which includes entities like Deployments, Changes, Incidents, Commits, and the relationships between them. For example, a Deployment record might include: deployment ID, service name, timestamp, status (success/fail), commit IDs included, change ticket ID (if any). An Incident record might include: incident ID, timestamps, severity, linked change or deployment ID, etc. The plugin will translate raw API data into this internal model for processing.
	•	The plugin should be able to cross-reference records: e.g., given a deployment (from Harness or ServiceNow change), find the commit (from GitHub) that triggered it; given an incident (from ServiceNow), find if there was a change/deployment just prior (or a linked change ID) to identify the causing deployment.
	•	If multiple commits are deployed together, the plugin should account for all of them in lead time calculations (potentially by taking the commit that was last to be merged as the representative lead time for that deploy). If multiple incidents occur from one deployment, each should count towards MTTR, but the deployment should count as one failure for CFR (the plugin might use unique deployment IDs to ensure one deployment doesn’t double-count in CFR).
	•	The integration layer should handle differences in data granularity. For instance, GitHub may provide commit-level info, ServiceNow provides deployment at a higher level (change requests), and Jira at the feature level. The plugin should reconcile these: e.g., if multiple Jira issues were part of one deployment, lead time might be calculated per issue or per commit. The requirements should state whether the metrics are at the aggregate level (overall performance of the service/team) – which is the usual approach – meaning these complexities are abstracted away by averaging or counting. The plugin will thus ensure each metric is computed consistently (e.g., median lead time per deployment or per change).
	•	Authentication & Access Control for Integrations: For each external system, the plugin must provide a means to configure authentication credentials and manage access:
	•	It should use Backstage’s configuration or secrets management to store API keys (for example, in app-config.yaml with secrets passed via environment variables or vault). These credentials should not be exposed on the frontend; the plugin’s backend will handle all API calls securely, ensuring that any secret (like a GitHub token or ServiceNow password) is never sent to the client or stored in logs.
	•	The plugin’s backend should also enforce that only authorized plugin clients (internal to Backstage) can trigger integration calls. This might involve using Backstage’s built-in proxy or backend plugin framework where the frontend calls a backend API route, and the backend carries out the external API call with stored credentials. This separation keeps credentials safe and allows central management of tokens.
	•	If applicable, support for user-level credentials can be a nice-to-have: e.g., if a user wants to use their personal GitHub OAuth token to see additional data. However, for simplicity, a single service account per integration is expected in this plugin.
	•	The integration must also respect authorization scopes: e.g., the ServiceNow account might only have access to a specific ServiceNow application or group of change tickets. The plugin should not assume it can see everything – it should handle “no access” responses gracefully (for example, if an incident is not visible due to permissions, skip it or notify). This ties back to security and compliance as well.
	•	Error Recovery and Logging: Each integration should implement robust error handling strategies. If GitHub’s API rate limit is hit, the plugin could back off and resume later, while logging a warning. If Jira’s API returns an error (like 403 Forbidden or 500 Server Error), the plugin should log the details and possibly alert an administrator if misconfiguration is suspected (e.g., invalid credentials). The requirements include having distinct logging for each integration to aid troubleshooting (for example, logging at info level when a sync succeeds and at error level with details when it fails). There should also be metrics or health checks for integrations – e.g., a heartbeat that indicates when each system was last successfully polled, which can be exposed via Backstage’s backend monitoring.

Visualization Requirements
	•	Dashboard Overview: The plugin will provide a DORA Metrics Dashboard in Backstage that presents an at-a-glance overview of the metrics. The layout should be clean and organized, typically using a grid of panels or cards. For each of the five metrics, there should be a dedicated visualization that includes:
	•	The metric name and current value (for the selected time period or context).
	•	An indication of the trend or change (for example, an arrow or percentage change since the previous period).
	•	Visual representation appropriate to the metric (e.g., a line chart over time for deployment frequency, showing deployments per day/week; a bar or line chart for trend in lead time; a gauge or pie for change failure rate percentage; etc.).
	•	Each metric panel should be easily identifiable and use intuitive visuals. For example, use a green upward arrow for improvement (like higher deployment frequency is good), or a red upward arrow for a worsening metric (like higher change failure rate is bad). Color-coding (with consideration for color-blind accessibility) should consistently indicate positive or negative performance.
	•	Trend Charts and Timeframe Selection: The dashboard must allow users to examine metrics over various timeframes. Users should be able to switch between views such as the last 7 days, last 30 days, last 90 days, or a custom date range. This could be implemented as a dropdown or tabs (e.g., “Week | Month | Quarter | Year to Date”). Changing the timeframe will update all metric displays to reflect data from that period ￼. Additionally, each metric’s panel might include a mini-chart: for instance, a sparkline of Deployment Frequency over the past weeks, or a trend line of MTTR over months. This helps users see not just the current value but the historical trajectory. The visualization should emphasize both throughput metrics (Deployment Frequency, Lead Time) and stability metrics (Change Failure Rate, MTTR, Rework) side by side, to encourage a balanced interpretation (as DORA research notes, elite teams excel in both speed and stability ￼).
	•	Drill-down and Detailed Views: While the high-level dashboard shows summary numbers, the UI should offer a way to drill down into details for each metric:
	•	Users could click on a metric card to open a detailed view or modal. For example, clicking on Deployment Frequency might show a detailed chart by day, and list each deployment in the selected period (with date/time and perhaps a link to the pipeline run or change record). Clicking on Change Failure Rate could list the specific deployments that failed, with links to incident reports or change logs.
	•	The detailed views should integrate with Backstage’s routing (possibly separate plugin pages or dialogs). In these views, provide context like: for Lead Time, perhaps show a distribution of lead times for individual commits or issues delivered; for MTTR, list recent incidents with their individual recovery times.
	•	Where possible, include hyperlinks to external tools for deeper investigation. For instance, an incident in the MTTR detail list could link to the incident in ServiceNow (opening in a new tab), and a commit or build could link to GitHub or Harness. This cross-navigation capability is important for usability, letting engineers quickly jump to root cause analysis.
	•	Ensure that navigating back and forth from the summary dashboard to detailed views is smooth (maintain state of chosen service and timeframe, for example).
	•	Contextual Filtering (Scope by Service/Team): The Backstage plugin should respect the context of the Backstage Software Catalog. Typically, Backstage organizes information by software component (service) and by team. The DORA metrics visualization should be able to operate in two modes:
	1.	Service (Component) Dashboard: When a user navigates to a specific service’s page in Backstage, they should see the DORA metrics for that service. For example, a service’s overview page could include a DORA Metrics section or tab that shows the five metrics specifically for that service (i.e. deployments of that service, incidents affecting that service, etc.). This requires that the plugin knows how to filter data per service – likely via the mappings (repo, Jira project, ServiceNow service) tied to that service’s catalog info. This way, developers can see how a particular application is performing on DORA metrics, alongside its other info like recent builds, alerts, docs (as Harness’s Backstage integration demonstrates, showing relevant info on the service’s homepage ￼).
	2.	Aggregate (Organization/Team) Dashboard: There should also be a global DORA metrics view available (for example, accessible via a Backstage sidebar item like “Engineering Metrics” or “DORA Dashboard”). This would show metrics at a higher level, such as an aggregate for the whole organization or filtered by team or business unit. Users could select a team or a group of services to see an overview (the plugin might leverage Backstage group entities or a custom selector). For instance, an engineering director might pick “Team A” vs “Team B” to compare their metrics.
The UI should make it easy to switch context. In a global dashboard, a dropdown to filter by team or service grouping is useful ￼. In a service-specific view, the context is implicit (the service currently viewed). The design will ensure it’s clear what scope the metrics are showing (e.g., display the service name or team name in the title of the dashboard).
	•	Comparison and Benchmarking: To add more insight, the visualization can support comparing metrics across different entities or against industry benchmarks:
	•	The plugin could allow users to compare two teams or services side by side on key metrics (for example, showing two sets of metric cards for Team X vs Team Y). This can highlight bottlenecks or outliers in performance ￼.
	•	It might also overlay industry benchmark values (such as DORA benchmark for elite performers) as a reference line on charts, if the data is available. For example, if elite teams have an MTTR of <1 hour, the MTTR chart could visually indicate that target.
	•	These comparison features are subject to data availability and might be provided in later iterations, but the requirements should note the desire for identifying gaps and bottlenecks via comparative visualization.
	•	UX and Layout Guidelines: The plugin’s UI should adhere to Backstage’s design system for consistency. Use the standard typography, cards, tables, and buttons as provided by Backstage’s core components. The layout should be responsive – on wider screens it might show all five metric panels in a grid, whereas on smaller screens it might stack them in a single column that can be scrolled. Key guidelines include:
	•	Clarity: Each metric panel must have a clear heading (the metric name) and not be cluttered. Auxiliary information (like last updated time, or a small icon indicating trend) should be present but not overwhelming.
	•	Tooltips/Help Text: Since not everyone is familiar with DORA metrics, include info tips. For example, an info icon next to “Change Failure Rate” can, on hover, show “Percentage of deployments causing a failure (e.g., requiring rollback)”. This educates users within the dashboard itself.
	•	Color and Icons: Use recognizable icons (e.g., a calendar or lightning bolt icon for Deployment Frequency, a stopwatch for Lead Time, a fire or bug icon for Failure Rate, a healing cross for MTTR, a redo arrow for Rework) if it adds clarity, but ensure they match the style guide. Use color coding sparingly to emphasize good vs poor performance, and ensure it meets contrast standards for accessibility.
	•	Accessibility: All interactive elements should have appropriate ARIA labels. Charts should have data point descriptions or at least be summarized in text form for screen readers (e.g., “Deployment Frequency: 5 deployments/week (average over last 4 weeks)”). Ensure that color is not the sole means of conveying information (use labels or patterns in charts as needed).
	•	Consistency: If the organization has existing dashboards or if Backstage Tech Insights/Scorecards are used, try to present DORA metrics in a consistent way. This may include similar layout or using the Tech Insights plugin (if available) to store and display these metrics, though the requirement is focused on a custom plugin in this case.
	•	Embedding and Navigation: The DORA dashboard should be easy to find and use within Backstage:
	•	Add a sidebar menu item for the global DORA metrics dashboard (for example, under an “Analytics” or “Metrics” section). This allows any user to navigate to the overview page ￼. On this overview, high-level aggregate metrics can be shown, possibly with the ability to filter by team.
	•	For service-specific pages, integrate the metrics as a tab or a section. Backstage allows adding custom tabs on an entity page. The requirement is to embed a DORA Metrics tab on a service’s page that shows that service’s metrics without the user having to navigate elsewhere. This embedded view should be a slimmed version focusing just on that service (though it could link to the full dashboard page if needed for more detail).
	•	The plugin should leverage Backstage routing such that each view (global or service-specific) is bookmarkable and refreshable. For example, a URL like /catalog/default/component/my-service/dora could show the metrics for “my-service”, and /dora could show the global dashboard. Deep linking is useful if managers want to share a link to the dashboard with others.
	•	Real-Time UI Updates: While not strictly required, it’s desirable that the dashboard reflect updates in near real-time when data changes. If a new deployment happens or an incident is resolved, the plugin could refresh the affected metrics on the fly (perhaps via a WebSocket or simply short polling intervals for the UI). At minimum, provide a “Refresh” button on the dashboard so users can manually fetch the latest metrics on demand. The UI should indicate the last data refresh timestamp so users know how current the information is.

In summary, the Harness DORA Metrics plugin will combine data from GitHub, Jira, and ServiceNow to present actionable insights via Backstage. It will fulfill functional needs of data gathering and metric computation, meet non-functional criteria like security and performance, integrate robustly with each external tool (ensuring proper authentication and mapping), and deliver a clear, user-friendly visualization of key DevOps metrics. By doing so, it enables engineering teams and leadership to continuously monitor and improve their software delivery performance using the DORA framework, all within their internal developer portal.  ￼ ￼ (The integration of multiple tools through this plugin provides a unified view that was otherwise missing when data was siloed in GitHub, Jira, and ServiceNow individually ￼. This Backstage dashboard becomes a one-stop location to observe software delivery throughput and stability, helping teams make informed decisions for improvement.)